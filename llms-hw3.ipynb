{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a5d61b",
   "metadata": {},
   "source": [
    "# PragmatiCQA With LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc209c9",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ee514",
   "metadata": {},
   "source": [
    "Before you begin coding, read the original paper that introduces the PRAGMATICQA dataset. In a few paragraphs:\n",
    "\n",
    "- Summarize the key motivations and contributions of the paper.\n",
    "- Explain in a qualitative manner what makes this dataset challenging for NLP models. What specific pragmatic phenomena does it target?\n",
    "- Select a few (about 5) sample conversations from the dataset (from different topics) and explain how the pragmatic answer enriches the literal answer that would be produced by a non-cooperative teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d7cc9",
   "metadata": {},
   "source": [
    "## The \"Traditional\" NLP Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b82cbba",
   "metadata": {},
   "source": [
    "### RAG Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0128d25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67e98fce3ec41aa9875aaa3304d0588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c594b5d1bb47e29c2845844e1d3e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/226 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d8be57e6334a428f2a57dc82e6f771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b79e27c5864fe09becaab8bce5fd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41af82688ec44cde9cd741a75290eecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0_StaticEmbedding/model.safetensors:   0%|          | 0.00/125M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dspy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load an extremely efficient local model for retrieval\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "\n",
    "# Create an embedder using the model's encode method\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "# Traverse a directory and read html files - extract text from the html files\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "def read_html_files(directory):\n",
    "    texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".html\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'html.parser')\n",
    "                texts.append(soup.get_text())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d104846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_characters = 10000  \n",
    "topk_docs_to_retrieve = 5  # Number of passages per query\n",
    "\n",
    "# Map topic name â†’ DSPy retriever\n",
    "topicToRetriever = {}\n",
    "\n",
    "sources_root = \"../PragmatiCQA-sources\"\n",
    "\n",
    "for topic in os.listdir(sources_root):\n",
    "    topic_path = os.path.join(sources_root, topic)\n",
    "    \n",
    "    if not os.path.isdir(topic_path):\n",
    "        continue  \n",
    "    \n",
    "    corpus = read_html_files(topic_path)\n",
    "    corpus = [doc[:max_characters] for doc in corpus]  # Truncate documents to avoid very long ones\n",
    "\n",
    "    retriever = dspy.retrievers.Embeddings(\n",
    "        embedder=embedder,\n",
    "        corpus=corpus,\n",
    "        k=topk_docs_to_retrieve\n",
    "    )\n",
    "\n",
    "    topicToRetriever[topic] = retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a RAG module with a given retriever.\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, search):\n",
    "        self.search = search\n",
    "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.search(question).passages\n",
    "        return self.respond(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"XAI_API_KEY\"] = \"\"\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2afa266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, retrieverMap = topicToRetriever):\n",
    "        self.retrieverMap = retrieverMap\n",
    "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "    def forward(self, topic, question):\n",
    "        if topic not in self.retrieverMap:\n",
    "            raise ValueError(f\"Topic '{topic}' not found in retriever map.\")\n",
    "        \n",
    "        # Gets the retriever for the specified topic \n",
    "        search = self.retrieverMap[topic]\n",
    "        # Retrieves relevant passages using appropriate retriever\n",
    "        context = search(question).passages\n",
    "        return self.respond(context=context, question=question)\n",
    "    \n",
    "rag = RAG(topicToRetriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b300371",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b96685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load jsonl from dataset directory\n",
    "import json \n",
    "\n",
    "def read_data(filename, dataset_dir=\"../PragmatiCQA/data\"):\n",
    "    corpus = []\n",
    "    with open(os.path.join(dataset_dir, filename), 'r') as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    return corpus\n",
    "\n",
    "pcqa_val = read_data(\"val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5120b446",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37631c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "# Instantiate the metric.\n",
    "metric = SemanticF1(decompositional=True)\n",
    "\n",
    "def evaluate_configs_batch(rag=rag, dataset=pcqa_val):\n",
    "\n",
    "    gold_examples = []\n",
    "    lit_preds = []\n",
    "    prag_preds = []\n",
    "    rag_preds = []\n",
    "\n",
    "    for doc in dataset:\n",
    "        topic = doc['topic']\n",
    "        qa = doc['qas'][0]\n",
    "        question = qa['q']\n",
    "        gold_ans = qa['a']\n",
    "        lit_answer = ' '.join([l['text'] for l in qa['a_meta']['literal_obj']])\n",
    "        prag_answer = ' '.join([l['text'] for l in qa['a_meta']['pragmatic_obj']])\n",
    "\n",
    "        # Prepare gold example\n",
    "        gold = dspy.Example(question=question, response=gold_ans)\n",
    "        gold_examples.append(gold)\n",
    "\n",
    "        # Create predictions\n",
    "        lit_preds.append(dspy.Example(question=question, response=lit_answer))\n",
    "        prag_preds.append(dspy.Example(question=question, response=prag_answer))\n",
    "        rag_preds.append(rag(topic, question))  \n",
    "\n",
    "    # Batched scoring\n",
    "    lit_scores = metric.batch(gold_examples, lit_preds)\n",
    "    prag_scores = metric.batch(gold_examples, prag_preds)\n",
    "    rag_scores = metric.batch(gold_examples, rag_preds)\n",
    "\n",
    "    return lit_scores, prag_scores, rag_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ee939",
   "metadata": {},
   "source": [
    "#### Literal Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eabae2",
   "metadata": {},
   "source": [
    "#### Pragmatic Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2a0626",
   "metadata": {},
   "source": [
    "#### Retrieved Context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
