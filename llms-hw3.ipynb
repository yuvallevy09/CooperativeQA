{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a5d61b",
   "metadata": {},
   "source": [
    "# PragmatiCQA With LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc209c9",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ee514",
   "metadata": {},
   "source": [
    "Before you begin coding, read the original paper that introduces the PRAGMATICQA dataset. In a few paragraphs:\n",
    "\n",
    "- Summarize the key motivations and contributions of the paper.\n",
    "- Explain in a qualitative manner what makes this dataset challenging for NLP models. What specific pragmatic phenomena does it target?\n",
    "- Select a few (about 5) sample conversations from the dataset (from different topics) and explain how the pragmatic answer enriches the literal answer that would be produced by a non-cooperative teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d7cc9",
   "metadata": {},
   "source": [
    "## The \"Traditional\" NLP Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b300371",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0128d25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67e98fce3ec41aa9875aaa3304d0588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c594b5d1bb47e29c2845844e1d3e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/226 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d8be57e6334a428f2a57dc82e6f771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b79e27c5864fe09becaab8bce5fd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41af82688ec44cde9cd741a75290eecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0_StaticEmbedding/model.safetensors:   0%|          | 0.00/125M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dspy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load an extremely efficient local model for retrieval\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "\n",
    "# Create an embedder using the model's encode method\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "# Traverse a directory and read html files - extract text from the html files\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "def read_html_files(directory):\n",
    "    texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".html\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'html.parser')\n",
    "                texts.append(soup.get_text())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d104846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_characters = 10000  \n",
    "topk_docs_to_retrieve = 5  # Number of passages per query\n",
    "\n",
    "# Map topic name → DSPy retriever\n",
    "topicToRetriever = {}\n",
    "\n",
    "sources_root = \"../PragmatiCQA-sources\"\n",
    "\n",
    "for topic in os.listdir(sources_root):\n",
    "    topic_path = os.path.join(sources_root, topic)\n",
    "    \n",
    "    if not os.path.isdir(topic_path):\n",
    "        continue  \n",
    "    \n",
    "    corpus = read_html_files(topic_path)\n",
    "    corpus = [doc[:max_characters] for doc in corpus]  # Truncate documents to avoid very long ones\n",
    "\n",
    "    retriever = dspy.retrievers.Embeddings(\n",
    "        embedder=embedder,\n",
    "        corpus=corpus,\n",
    "        k=topk_docs_to_retrieve\n",
    "    )\n",
    "\n",
    "    topicToRetriever[topic] = retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c26db43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86dedf55c2048909efd6fffc9bea567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09b4d48a8084c2386a8c809dc4537d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84ba28a38c747068779c342315e5e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d6302f957c40e9ad9172ec62044db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43fa36cdca1433e93a4af9c52496a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "qa_model = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b96685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load jsonl from dataset directory\n",
    "import json \n",
    "\n",
    "def read_data(filename, dataset_dir=\"../PragmatiCQA/data\"):\n",
    "    corpus = []\n",
    "    with open(os.path.join(dataset_dir, filename), 'r') as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    return corpus\n",
    "\n",
    "pcqa_val = read_data(\"val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5120b446",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37631c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "metric = SemanticF1(decompositional=True)\n",
    "\n",
    "def evaluate_configs_batch(dataset=pcqa_val, retrieverMap=topicToRetriever, model=qa_model):\n",
    "\n",
    "    gold_examples = []\n",
    "    lit_preds = []\n",
    "    prag_preds = []\n",
    "    model_preds = []\n",
    "\n",
    "    for i, doc in enumerate(dataset):\n",
    "        topic = doc['community']\n",
    "        if topic not in retrieverMap:\n",
    "            print(f\"Skipping topic: {topic} (not found in retriever map)\")\n",
    "            continue\n",
    "        print(f\"Processing doc number {i} with topic: {topic}\")\n",
    "        qa = doc['qas'][0]\n",
    "        question = qa['q']\n",
    "        gold_ans = qa['a']\n",
    "\n",
    "        # Extract contexts for model\n",
    "        lit_context = ' '.join([l['text'] for l in qa['a_meta']['literal_obj']])\n",
    "        prag_context = ' '.join([l['text'] for l in qa['a_meta']['pragmatic_obj']])\n",
    "        model_context = ' '.join(retrieverMap[topic](question).passages)\n",
    "        \n",
    "        # Get answers from model\n",
    "        lit_answer = model(question=question, context=lit_context)['answer']\n",
    "        prag_answer = model(question=question, context=prag_context)['answer']\n",
    "        model_answer = model(question=question, context=model_context)['answer']\n",
    "\n",
    "        # Prepare gold example\n",
    "        gold = dspy.Example(question=question, response=gold_ans).with_inputs(\"question\")\n",
    "        gold_examples.append(gold)\n",
    "\n",
    "        # Add answers to preds\n",
    "        lit_preds.append(dspy.Example(question=question, response=lit_answer).with_inputs(\"question\"))\n",
    "        prag_preds.append(dspy.Example(question=question, response=prag_answer).with_inputs(\"question\"))\n",
    "        model_preds.append(dspy.Example(question=question, response=model_answer).with_inputs(\"question\"))  \n",
    "\n",
    "    # Batched scoring\n",
    "    lit_scores = SemanticF1.batch(gold_examples, lit_preds)\n",
    "    prag_scores = SemanticF1.batch(gold_examples, prag_preds)\n",
    "    model_scores = SemanticF1.batch(gold_examples, model_preds)\n",
    "\n",
    "    return lit_scores, prag_scores, model_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b26aa949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compare_results(lit_scores, prag_scores, retrieved_scores, dataset=pcqa_val):\n",
    "    rows = []\n",
    "\n",
    "    for i, doc in enumerate(dataset):\n",
    "        topic = doc['topic']\n",
    "        qa = doc['qas'][0]\n",
    "        question = qa['q']\n",
    "        gold_answer = qa['a']\n",
    "\n",
    "        rows.append({\n",
    "            \"Topic\": topic,\n",
    "            \"Question\": question,\n",
    "            \"Gold Answer\": gold_answer,\n",
    "            \"Literal F1\": lit_scores[i],\n",
    "            \"Pragmatic F1\": prag_scores[i],\n",
    "            \"Retrieved F1\": retrieved_scores[i]\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Print the average scores\n",
    "    print(\"=== AVERAGE SEMANTIC F1 SCORES ===\")\n",
    "    print(f\"Literal:   {np.mean(df['Literal F1']):.3f}\")\n",
    "    print(f\"Pragmatic: {np.mean(df['Pragmatic F1']):.3f}\")\n",
    "    print(f\"Retrieved: {np.mean(df['Retrieved F1']):.3f}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a58f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing doc number 0 with topic: A Nightmare on Elm Street\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/04 19:30:30 ERROR dspy.utils.parallelizer: Error for ([Example({'question': 'who is freddy krueger?', 'response': \"Freddy Kruger is the nightmare in nighmare on Elm street. Please note, and to be very clear, the system that loads up wiki is not allowing access to Adam Prag, to the page... so I'll have to go from memory.  Normally you can paste things and back up what you are saying, but today that's not happening. alas.\"}) (input_keys={'question'})], Example({'question': 'who is freddy krueger?'}) (input_keys={'question'})): 'list' object is not callable. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 / 1 examples: 100%|██████████| 1/1 [00:00<00:00, 1408.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/04 19:30:30 ERROR dspy.utils.parallelizer: Error for ([Example({'question': 'who is freddy krueger?', 'response': \"Freddy Kruger is the nightmare in nighmare on Elm street. Please note, and to be very clear, the system that loads up wiki is not allowing access to Adam Prag, to the page... so I'll have to go from memory.  Normally you can paste things and back up what you are saying, but today that's not happening. alas.\"}) (input_keys={'question'})], Example({'question': 'who is freddy krueger?'}) (input_keys={'question'})): 'list' object is not callable. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 0 / 1 examples: 100%|██████████| 1/1 [00:00<00:00, 1615.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/04 19:30:30 ERROR dspy.utils.parallelizer: Error for ([Example({'question': 'who is freddy krueger?', 'response': \"Freddy Kruger is the nightmare in nighmare on Elm street. Please note, and to be very clear, the system that loads up wiki is not allowing access to Adam Prag, to the page... so I'll have to go from memory.  Normally you can paste things and back up what you are saying, but today that's not happening. alas.\"}) (input_keys={'question'})], Example({'question': 'who is freddy krueger?'}) (input_keys={'question'})): 'list' object is not callable. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 0 / 1 examples: 100%|██████████| 1/1 [00:00<00:00, 2688.66it/s]\n",
      "=== Example Evaluation ===\n",
      "Topic:        A Nightmare on Elm Street (2010 film)\n",
      "Question:     who is freddy krueger?\n",
      "Gold Answer:  Freddy Kruger is the nightmare in nighmare on Elm street. Please note, and to be very clear, the system that loads up wiki is not allowing access to Adam Prag, to the page... so I'll have to go from memory.  Normally you can paste things and back up what you are saying, but today that's not happening. alas.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuestion:     \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_doc[\u001b[33m'\u001b[39m\u001b[33mqas\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mq\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGold Answer:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_doc[\u001b[33m'\u001b[39m\u001b[33mqas\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLiteral F1:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlit_scores[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPragmatic F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprag_scores[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrieved F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_scores[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "# Test evaluate_configs_batch on a single example\n",
    "test_doc = pcqa_val[0]\n",
    "test_dataset = [test_doc]\n",
    "\n",
    "# Run evaluation on the first example only\n",
    "lit_scores, prag_scores, model_scores = evaluate_configs_batch(dataset=test_dataset)\n",
    "\n",
    "# Print results\n",
    "print(\"=== Example Evaluation ===\")\n",
    "print(f\"Topic:        {test_doc['topic']}\")\n",
    "print(f\"Question:     {test_doc['qas'][0]['q']}\")\n",
    "print(f\"Gold Answer:  {test_doc['qas'][0]['a']}\")\n",
    "print(f\"Literal F1:   {lit_scores[0]:.3f}\")\n",
    "print(f\"Pragmatic F1: {prag_scores[0]:.3f}\")\n",
    "print(f\"Retrieved F1: {model_scores[0]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2afa266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, retrieverMap = topicToRetriever):\n",
    "        self.retrieverMap = retrieverMap\n",
    "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "    def forward(self, topic, question):\n",
    "        if topic not in self.retrieverMap:\n",
    "            raise ValueError(f\"Topic '{topic}' not found in retriever map.\")\n",
    "        \n",
    "        # Gets the retriever for the specified topic \n",
    "        search = self.retrieverMap[topic]\n",
    "        # Retrieves relevant passages using appropriate retriever\n",
    "        context = search(question).passages\n",
    "        return self.respond(context=context, question=question)\n",
    "    \n",
    "rag = RAG(topicToRetriever)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
